{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abiralchy0987/movie_recommendation_system/blob/main/Full_recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C90vVBxWjttz",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "1f34f4ca-6c1c-4864-dc67-5aaff0432a12"
   },
   "outputs": [],
   "source": [
    "# Content filtering\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "\n",
    "#reading the data\n",
    "movies = pd.read_csv('/content/tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('/content/tmdb_5000_credits.csv')\n",
    "\n",
    "movies.head(2)\n",
    "\n",
    "#head shows only one row of the dataset because of the parameter (1)\n",
    "credits.head(1)\n",
    "\n",
    "movies.shape\n",
    "\n",
    "#shape shows how many rows and columns are available in the dataset\n",
    "credits.shape\n",
    "\n",
    "#integration of datasets movies and credits\n",
    "movies = movies.merge(credits , on = 'title')\n",
    "\n",
    "movies.head(1)\n",
    "\n",
    "#visualising the integrated datasets\n",
    "#previously movies had 20 columns and credits had four columns and since\n",
    "#it was integrated on the column \"title\" which was same on both the datasets\n",
    "#the new number of columns present is 23\n",
    "movies.shape\n",
    "\n",
    "\n",
    "movies.columns\n",
    "\n",
    "#cleaning data\n",
    "movies = movies[['movie_id','title','overview','genres','keywords','cast','crew']]\n",
    "\n",
    "#data preprocessing\n",
    "#checking for missing values\n",
    "movies.isnull().sum()\n",
    "\n",
    "#dropping the rows with missing value because there is very small number of data with missing value\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "movies.duplicated().sum()\n",
    "\n",
    "movies.iloc[0]['genres']\n",
    "\n",
    "import ast\n",
    "def convert(text):\n",
    "  l=[]\n",
    "  for i in ast.literal_eval(text):\n",
    "    l.append(i['name'])\n",
    "\n",
    "  return l\n",
    "\n",
    "\n",
    "movies['genres']=movies['genres'].apply(convert)\n",
    "\n",
    "movies['keywords']=movies['keywords'].apply(convert)\n",
    "\n",
    "import ast\n",
    "def convert_cast(text):\n",
    "  l=[]\n",
    "  counter=0\n",
    "  for i in ast.literal_eval(text):\n",
    "    if counter<3:\n",
    "     l.append(i['name'])\n",
    "    counter+=1\n",
    "\n",
    "  return l\n",
    "\n",
    "movies['cast']=movies['cast'].apply(convert_cast)\n",
    "\n",
    "import ast\n",
    "def fetch_director(text):\n",
    "  l=[]\n",
    "  for i in ast.literal_eval(text):\n",
    "    if i['job'] == 'Director':\n",
    "     l.append(i['name'])\n",
    "     break\n",
    "\n",
    "  return l\n",
    "\n",
    "movies['crew']=movies['crew'].apply(fetch_director)\n",
    "\n",
    "\n",
    "\n",
    "movies['overview']=movies['overview'].apply(lambda x:x.split())\n",
    "\n",
    "def remove_space(word):\n",
    "  l = []\n",
    "  for i in word:\n",
    "    l.append(i.replace(\" \",\"\"))\n",
    "  return l\n",
    "\n",
    "\n",
    "movies['cast']=movies['cast'].apply(remove_space)\n",
    "\n",
    "movies['crew']=movies['crew'].apply(remove_space)\n",
    "\n",
    "movies['tags']= movies['overview']+movies['keywords']+movies['genres']+movies['cast']+movies['crew']\n",
    "\n",
    "new_df = movies[['movie_id','title','tags']]\n",
    "\n",
    "new_df['tags'] = new_df['tags'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "new_df['tags'] = new_df['tags'].apply(lambda x:x.lower())\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem(text):\n",
    "  l = []\n",
    "  for i in text.split():\n",
    "    l.append(ps.stem(i))\n",
    "\n",
    "  return \" \".join(l)\n",
    "\n",
    "new_df['tags'] = new_df['tags'].apply(stem)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "words_clean=[]\n",
    "for word in new_df['tags']:\n",
    " stopwords = nltk.corpus.stopwords.words('english')\n",
    "if word not in stopwords:\n",
    "     l = []\n",
    "for i in word.split():\n",
    "     l.append(ps.stem(i))\n",
    "     words_clean.append(\" \".join(l))\n",
    "else:\n",
    "         words_clean.append(word)\n",
    "         print(words_clean)\n",
    "# def stem(text):\n",
    "#   l = []\n",
    "#   for i in text.split():\n",
    "#     l.append(ps.stem(i))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000,stop_words='english')\n",
    "vector= cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vector)\n",
    "similarity\n",
    "\n",
    "new_df[new_df['title'] == 'Spider-Man'].index[0]\n",
    "\n",
    "def recommend(movie):\n",
    "  movie_index = new_df[new_df['title'] == movie].index[0]\n",
    "  distances =sorted(list(enumerate(similarity[movie_index])),reverse=True, key = lambda x: x[1])\n",
    "  for i in distances[1:6]:\n",
    "    print(new_df.iloc[i[0]].title)\n",
    "\n",
    "\n",
    "recommend('Spider-Man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3MwMeX6jovdd",
    "outputId": "db546672-d3af-4a53-a9d0-eec237acb6b8"
   },
   "outputs": [],
   "source": [
    "#collaborative filtering\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "movies_df = pd.read_csv('movies.csv')\n",
    "ratings_df = pd.read_csv('ratings.csv')\n",
    "\n",
    "ratings_df.shape\n",
    "\n",
    "movies_df.head(1)\n",
    "\n",
    "ratings_df.head(1)\n",
    "\n",
    "movies = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "\n",
    "\n",
    "# Remove duplicates if any\n",
    "movies = movies.drop_duplicates()\n",
    "\n",
    "\n",
    "# Create the User-Item Rating Matrix (pivot table)\n",
    "user_item_matrix = movies.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Display the shape and first few entries of the matrix\n",
    "print(user_item_matrix.shape)\n",
    "print(user_item_matrix.head())\n",
    "\n",
    "\n",
    "# Calculate the global average rating\n",
    "global_avg_rating = movies['rating'].mean()\n",
    "\n",
    "# Fill NaN values with the global average rating\n",
    "user_item_matrix_filled = user_item_matrix.fillna(global_avg_rating)\n",
    "\n",
    "\n",
    "user_item_matrix_filled.head(1)\n",
    "\n",
    "%pip install scikit-surprise\n",
    "\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(movies[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Initialize SVD model\n",
    "svd = SVD()\n",
    "\n",
    "# Train the model\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Predict ratings on the testset\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "\n",
    "# Predict the rating for a specific user-item pair\n",
    "user_id = 1  # Example user\n",
    "movie_id = 50  # Example movie\n",
    "\n",
    "predicted_rating = svd.predict(user_id, movie_id).est\n",
    "print(f\"Predicted rating for User {user_id} on Movie {movie_id}: {predicted_rating}\")\n",
    "\n",
    "\n",
    "import heapq\n",
    "\n",
    "def recommend_movies(user_id, top_n=10):\n",
    "    \"\"\"Recommends top N movies for a specific user.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The ID of the user.\n",
    "        top_n (int, optional): The number of movies to recommend. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of movie titles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all movie IDs and create a dictionary for movieId -> title lookup\n",
    "    movie_id_to_title = dict(zip(movies['movieId'], movies['title']))\n",
    "\n",
    "    # Get all unique movie IDs\n",
    "    all_movie_ids = movies['movieId'].unique()\n",
    "\n",
    "    # Get the movies that the user has already rated\n",
    "    rated_movies = set(movies[movies['userId'] == user_id]['movieId'])\n",
    "\n",
    "    # Get the unrated movies\n",
    "    unrated_movies = [movie_id for movie_id in all_movie_ids if movie_id not in rated_movies]\n",
    "\n",
    "    # Predict ratings for unrated movies\n",
    "    predictions = [svd.predict(user_id, movie_id) for movie_id in unrated_movies]\n",
    "\n",
    "    # Sort movies by predicted rating (highest first)\n",
    "    recommended_movies = heapq.nlargest(top_n, predictions, key=lambda x: x.est)\n",
    "\n",
    "    # Get movie titles from the top N predictions using the dictionary\n",
    "    movie_titles = [movie_id_to_title[rec.iid] for rec in recommended_movies]\n",
    "\n",
    "    return movie_titles\n",
    "\n",
    "# Example usage:\n",
    "user_id = 1  # Example user ID\n",
    "recommended_movies = recommend_movies(user_id, top_n=10)\n",
    "print(f\"Top 10 movie recommendations for User {user_id}: {recommended_movies}\")\n",
    "\n",
    "user_id_input = input(\"Enter your user ID: \")\n",
    "try:\n",
    "    user_id = int(user_id_input)  # Ensure the user ID is an integer\n",
    "    recommended_movies = recommend_movies(user_id, top_n=10)\n",
    "    print(f\"Top 10 movie recommendations for User {user_id}: {recommended_movies}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a valid integer for user ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zqH3EcT1srnL",
    "outputId": "bceba9a1-aa88-44e3-a17c-ed16cd662c58"
   },
   "outputs": [],
   "source": [
    "%pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxxdG6frtScZ",
    "outputId": "07b55aad-1c87-4a8e-e3ea-992a85aeb5e3"
   },
   "outputs": [],
   "source": [
    "from thefuzz import fuzz, process\n",
    "\n",
    "def find_closest_movie(title_input):\n",
    "    \"\"\"\n",
    "    Finds the closest matching movie title in new_df using fuzzy matching.\n",
    "    Both the user input and the titles are compared in lowercase.\n",
    "    \"\"\"\n",
    "    all_titles = new_df['title'].tolist()\n",
    "    # Create a lowercase version for fuzzy matching\n",
    "    all_titles_lower = [t.lower() for t in all_titles]\n",
    "    title_input_lower = title_input.lower().strip()\n",
    "\n",
    "    # Get the best match using fuzzy matching\n",
    "    best_match, score = process.extractOne(title_input_lower, all_titles_lower, scorer=fuzz.partial_ratio)\n",
    "\n",
    "    # Accept the match if the score is above a threshold (e.g., 60)\n",
    "    if score > 60:\n",
    "        # Return the original title (with proper casing) corresponding to the best match\n",
    "        match_index = all_titles_lower.index(best_match)\n",
    "        return new_df.iloc[match_index]['title']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def hybrid_recommend(user_id, movie_title, top_n=10):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation that returns half the recommendations from content-based filtering\n",
    "    and half from collaborative filtering.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The ID of the user (for collaborative filtering).\n",
    "        movie_title (str): The movie title used for content-based filtering (input is fuzzy-matched).\n",
    "        top_n (int): Total number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recommended movie titles.\n",
    "    \"\"\"\n",
    "    # Determine how many movies to take from each recommender\n",
    "    n_content = top_n // 2\n",
    "    n_collab = top_n - n_content  # This ensures if top_n is odd, collaborative gets the extra slot\n",
    "\n",
    "    # ----------- Content-Based Filtering -----------\n",
    "    # Find the closest matching movie title\n",
    "    matched_movie = find_closest_movie(movie_title)\n",
    "    if not matched_movie:\n",
    "        print(f\"Sorry, we couldn't find a close match for '{movie_title}'. Please try another title.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Using content-based match: '{matched_movie}'\")\n",
    "\n",
    "    # Get the index for the matched movie from new_df\n",
    "    try:\n",
    "        movie_index = new_df[new_df['title'] == matched_movie].index[0]\n",
    "    except IndexError:\n",
    "        print(f\"Movie '{matched_movie}' not found in the dataset.\")\n",
    "        return []\n",
    "\n",
    "    # Get similarity scores for all movies relative to the matched movie\n",
    "    content_scores = list(enumerate(similarity[movie_index]))\n",
    "    # Sort scores in descending order (most similar first)\n",
    "    content_scores = sorted(content_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Build content-based recommendation list (skip the first entry as it is the movie itself)\n",
    "    content_recs = []\n",
    "    for idx, score in content_scores[1:]:\n",
    "        rec_title = new_df.iloc[idx]['title']\n",
    "        if rec_title not in content_recs:\n",
    "            content_recs.append(rec_title)\n",
    "        if len(content_recs) >= n_content:\n",
    "            break\n",
    "\n",
    "    # ----------- Collaborative Filtering -----------\n",
    "    # Get a larger pool from collaborative filtering so we can remove any duplicates later\n",
    "    collab_pool = recommend_movies(user_id, top_n=top_n * 2)\n",
    "    # Remove any movies that already appeared in the content-based recommendations\n",
    "    collab_recs = [m for m in collab_pool if m not in content_recs]\n",
    "    # Select the top n_collab recommendations\n",
    "    collab_recs = collab_recs[:n_collab]\n",
    "\n",
    "    # ----------- Merge Recommendations -----------\n",
    "    final_recs = content_recs + collab_recs\n",
    "\n",
    "    # In case there are not enough unique recommendations, try to fill from the collaborative pool\n",
    "    if len(final_recs) < top_n:\n",
    "        extra_needed = top_n - len(final_recs)\n",
    "        extra_from_collab = [m for m in collab_pool if m not in final_recs]\n",
    "        final_recs += extra_from_collab[:extra_needed]\n",
    "\n",
    "    return final_recs\n",
    "\n",
    "# ---------------------------\n",
    "# Example Usage\n",
    "# ---------------------------\n",
    "\n",
    "# Get user input\n",
    "try:\n",
    "    user_id = int(input(\"Enter your user ID: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid user ID. Please enter an integer value.\")\n",
    "    user_id = None\n",
    "\n",
    "if user_id is not None:\n",
    "    movie_title = input(\"Enter a movie title you like: \")\n",
    "\n",
    "    recommendations = hybrid_recommend(user_id, movie_title, top_n=10)\n",
    "\n",
    "    if recommendations:\n",
    "        print(\"\\nHybrid Recommendations (50% Content-Based, 50% Collaborative):\")\n",
    "        for i, title in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb9CkkkKqXtq",
    "outputId": "5b17500f-a9ab-4a8e-d9c3-d747deffb11b"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_movies, relevant_movies, k):\n",
    "    \"\"\"\n",
    "    Calculate precision@k for content-based recommendations.\n",
    "\n",
    "    Args:\n",
    "        recommended_movies (list): List of recommended movie titles.\n",
    "        relevant_movies (list): List of relevant movie titles (ground truth).\n",
    "        k (int): Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision@k.\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_movies[:k]\n",
    "    relevant_set = set(relevant_movies)\n",
    "    hits = sum(1 for movie in recommended_at_k if movie in relevant_set)\n",
    "    return hits / k\n",
    "\n",
    "# Example usage:\n",
    "relevant_movies = ['The Dark Knight', 'Inception', 'Interstellar']  # Ground truth\n",
    "recommended_movies = ['The Dark Knight', 'Interstellar', 'Avatar', 'The Matrix', 'Inception']\n",
    "k = 5\n",
    "precision = precision_at_k(recommended_movies, relevant_movies, k)\n",
    "print(f\"Precision@{k}: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "collapsed": true,
    "id": "MRXaHPiCxltb",
    "outputId": "8094fbff-270d-4134-f689-bf87d0ef996a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example RMSE values for different models\n",
    "models = ['SVD', 'KNN', 'NMF']\n",
    "rmse_values = [0.85, 0.90, 0.88]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, rmse_values, color=['blue', 'green', 'red'])\n",
    "plt.title('RMSE Comparison for Different Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "collapsed": true,
    "id": "ubtELjOExtKb",
    "outputId": "33d22405-14a0-4f79-9a92-67c01eb1214a"
   },
   "outputs": [],
   "source": [
    "# Example precision@k values for different k\n",
    "k_values = [5, 10, 15]\n",
    "precision_values = [0.6, 0.5, 0.4]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(k_values, precision_values, color='blue')\n",
    "plt.title('Precision@k for Content-Based Recommendations')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "esvlQGkaxy4l",
    "outputId": "47fb836e-3d4f-44e5-def0-6a4ad8661dc8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "models = ['Content-Based', 'Collaborative', 'Hybrid']\n",
    "precision_values = [0.6, 0.7, 0.75]\n",
    "rmse_values = [0.85, 0.90, 0.88]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, precision_values, width, label='Precision@10')\n",
    "rects2 = ax.bar(x + width/2, rmse_values, width, label='RMSE')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Content-Based, Collaborative, and Hybrid Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "SXdpAo6-x29E",
    "outputId": "b50e9e23-5133-4734-ef1f-42ed0f4c19f4"
   },
   "outputs": [],
   "source": [
    "# Example hybrid model evaluation\n",
    "hybrid_precision = 0.75\n",
    "hybrid_rmse = 0.88\n",
    "\n",
    "# Plotting\n",
    "models = ['Content-Based', 'Collaborative', 'Hybrid']\n",
    "precision_values = [0.6, 0.7, hybrid_precision]\n",
    "rmse_values = [0.85, 0.90, hybrid_rmse]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, precision_values, width, label='Precision@10')\n",
    "rects2 = ax.bar(x + width/2, rmse_values, width, label='RMSE')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Content-Based, Collaborative, and Hybrid Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
